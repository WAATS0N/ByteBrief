"""
Main ByteBrief News Scraper Application
"""
import sys
from pathlib import Path
from loguru import logger
from datetime import datetime

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.scraper.sources.bbc_scraper import BBCScraper
from src.scraper.sources.cnn_scraper import CNNScraper
from src.scraper.models import Article


def setup_logging():
    """Configure logging for the scraper"""
    # Remove default logger
    logger.remove()
    
    # Add console logging with colors
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    # Add file logging
    logs_dir = project_root / "logs"
    logs_dir.mkdir(exist_ok=True)
    
    logger.add(
        logs_dir / "bytebrief_{time:YYYY-MM-DD}.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}",
        level="DEBUG",
        rotation="1 day",
        retention="7 days"
    )


def test_bbc_scraper():
    """Test the BBC scraper functionality"""
    logger.info("ðŸš€ Starting ByteBrief BBC News Scraper Test")
    
    try:
        # Initialize BBC scraper
        scraper = BBCScraper()
        logger.info("âœ… BBC Scraper initialized successfully")
        
        # Test scraping BBC news
        logger.info("ðŸ“° Starting to scrape BBC News...")
        articles = scraper.scrape_source('bbc')
        
        if articles:
            logger.success(f"âœ… Successfully scraped {len(articles)} articles from BBC")
            
            # Display first article as sample
            if articles:
                sample_article = articles[0]
                logger.info("ðŸ“„ Sample Article:")
                logger.info(f"  Title: {sample_article.title}")
                logger.info(f"  URL: {sample_article.url}")
                logger.info(f"  Author: {sample_article.author}")
                logger.info(f"  Source: {sample_article.source}")
                logger.info(f"  Scraped: {sample_article.scraped_date}")
            
            return articles
            
        else:
            logger.warning("âš ï¸ No articles were scraped from BBC")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Error during BBC scraping: {e}")
        return []


def test_cnn_scraper():
    """Test the CNN scraper functionality"""
    logger.info("ðŸš€ Starting ByteBrief CNN News Scraper Test")
    
    try:
        # Initialize CNN scraper
        scraper = CNNScraper()
        logger.info("âœ… CNN Scraper initialized successfully")
        
        # Test scraping CNN news
        logger.info("ðŸ“° Starting to scrape CNN News...")
        articles = scraper.scrape_source('cnn')
        
        if articles:
            logger.success(f"âœ… Successfully scraped {len(articles)} articles from CNN")
            
            # Display first article as sample
            if articles:
                sample_article = articles[0]
                logger.info("ðŸ“„ Sample CNN Article:")
                logger.info(f"  Title: {sample_article.title}")
                logger.info(f"  URL: {sample_article.url}")
                logger.info(f"  Source: {sample_article.source}")
                logger.info(f"  Published: {sample_article.published_date}")
                logger.info(f"  Scraped: {sample_article.scraped_date}")
            
            return articles
            
        else:
            logger.warning("âš ï¸ No articles were scraped from CNN")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Error during CNN scraping: {e}")
        return []


def scrape_all_sources():
    """Scrape articles from all available news sources"""
    logger.info("ðŸŒ Starting multi-source news scraping...")
    
    all_articles = []
    
    # Scrape BBC News
    logger.info("" + "="*50)
    logger.info("ðŸ‡¬ðŸ‡§ SCRAPING BBC NEWS")
    logger.info("" + "="*50)
    bbc_articles = test_bbc_scraper()
    all_articles.extend(bbc_articles)
    
    logger.info("")
    
    # Scrape CNN News
    logger.info("" + "="*50)
    logger.info("ðŸ‡ºðŸ‡¸ SCRAPING CNN NEWS")
    logger.info("" + "="*50)
    cnn_articles = test_cnn_scraper()
    all_articles.extend(cnn_articles)
    
    # Summary and save combined results
    logger.info("")
    logger.info("" + "="*50)
    logger.info("ðŸ“Š SCRAPING SUMMARY")
    logger.info("" + "="*50)
    logger.info(f"ðŸ‡¬ðŸ‡§ BBC Articles: {len(bbc_articles)}")
    logger.info(f"ðŸ‡ºðŸ‡¸ CNN Articles: {len(cnn_articles)}")
    logger.info(f"ðŸ“ˆ Total Articles: {len(all_articles)}")
    
    if all_articles:
        # Save all articles to a combined file
        scraper = BBCScraper()  # Use any scraper instance for saving
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = scraper.save_articles(all_articles, f"all_news_{timestamp}.json")
        logger.success(f"ðŸ’¾ All articles saved to: {output_file}")
    
    return all_articles


def main():
    """Main application entry point"""
    setup_logging()
    
    logger.info("ðŸ—žï¸ ByteBrief News Scraper Starting")
    logger.info(f"ðŸ“… Current time: {datetime.now()}")
    
    try:
        # Scrape from all available sources (BBC + CNN)
        articles = scrape_all_sources()
        
        if articles:
            logger.success(f"âœ… Scraping completed successfully! Total articles: {len(articles)}")
        else:
            logger.warning("âš ï¸ No articles were scraped from any source")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸ Scraping interrupted by user")
        
    except Exception as e:
        logger.error(f"âŒ Fatal error: {e}")
        sys.exit(1)
    
    logger.info("ðŸ‘‹ ByteBrief News Scraper finished")


if __name__ == "__main__":
    main()
